Shadow is an experimental framework for an AI-first editor designed around formatting and semantic extraction. It is inspired by work I've done in [foolpy](https://github.com/alexezh/foolpy) focusing on off-the-shelf models. 

# Formatting as Semantic

Formatting is not only visual but also conveys meaning. By linking formatting to semantic intent, Shadow can recognize when a specific semantic element appears in another document and apply consistent formatting automatically.

# Semantic-Driven Editing

Shadow identifies and extracts structural and semantic details from existing documents, then leverages that knowledge to guide editing, ensuring both semantic clarity and stylistic consistency. 

For instance, if a user consistently creates tables with a blue header, Shadow will recognize this as a stylistic pattern and automatically apply the same design when the user inserts a new table. Similarly, when a user drafts a document that is semantically similar to one already in the library, Shadow can reuse the corresponding layout to maintain coherence and efficiency.

# Key design principles
A lot of things which users do boil down to context. If a user creates a document or makes a change to the existing document, the safe bet is to find something similar which a user has done in the past and reuse. The key then is to have a system which can collect such details in a flexible way - aka associative memory. 

Since we have to work outside LLM, and we want system to quickly adapt to a user, we cannot rely on training. Instead, we are going to split problem into multiple layers and expose several sets of APIs to LLM. 

## Blueprint
The first category is blueprint. For most of documents, a user only uses limited set of "design" patterns which we call blueprint. Blueprint specifies formatting for semantic elements of the document

```text
#### Character Names & Emphasis
- character.names: Mira, Percy (first mention)
- Format: <strong style="color:#278c28;"> (bold, deep green)
- Purpose: Visually distinguishes main characters on introduction
```    

The assumption is that a user will probably use the same formatting for the same semantic elements in similar documents. And if we can store and retrieve such elements, LLM can do better work on formatting. Needless to say that blueprints are generated by LLM automatically. 

## Embeddings
Through the system, we are going to rely on LLM to provide keywords for storing and retrieving data. such as API for storing context defined as

```text
  {
    type: 'function' as const,
    function: {
      name: 'set_context',
      description: 'Set context value based on terms. The terms will be used to look up the context name, then store the value.',
      parameters: {
        type: 'object',
        properties: {
          terms: {
            type: 'array',
            keywords: { type: 'string' },
            description: 'Keywords to identify which context to set (e.g., ["document_name"], ["current_file"])'
          },
          value: {
            type: 'string',
            description: 'The value to store in the context'
          }
        },
        required: ['terms', 'value']
      }
    }
  }
```
runtime code goes through keyword, generates embedding for each and stores value using embedding as a key. 

The interesting question is how to find the best match given multiple keywords. The simplest is to find the best match for each keyword, and then lookup the best across all. However this approach does not account for history.

We want a simple way to map uncertain facts to the most likely outcome.

* Let **S** be a set of facts **F₁…Fₖ**, where each fact is a keyword with an associated probability.
* Our prior knowledge is a list of **rules** (statements) of the form **Sᵢ ⇒ Yᵢ**, where **Y** is the predicted outcome.
* **Goal:** given a new set of facts **S**, compute the most probable **Y**.

See [rulemodel.md](rulemodel.md) for the full fact model and implementation details.
