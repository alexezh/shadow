Shadow is an experimental framework for an AI-first editor designed around formatting and semantic extraction. It is inspired by work I've done in [foolpy](https://github.com/alexezh/foolpy) focusing on off-the-shelf models. 

# Formatting as Semantic

Formatting is not only visual but also conveys meaning. By linking formatting to semantic intent, Shadow can recognize when a specific semantic element appears in another document and apply consistent formatting automatically.

# Semantic-Driven Editing

Shadow identifies and extracts structural and semantic details from existing documents, then leverages that knowledge to guide editing, ensuring both semantic clarity and stylistic consistency. 

For instance, if a user consistently creates tables with a blue header, Shadow will recognize this as a stylistic pattern and automatically apply the same design when the user inserts a new table. Similarly, when a user drafts a document that is semantically similar to one already in the library, Shadow can reuse the corresponding layout to maintain coherence and efficiency.

# Key design principles
A lot of things which users do boil down to context. If a user creates a document or makes a change to the existing document, the safe bet is to find something similar which a user has done in the past and reuse. The key then is to have a system which can collect such details in a flexible way - aka associative memory. 

Since we have to work outside LLM, and we want system to quickly adapt to a user, we cannot rely on training. Instead, we are going to split problem into multiple layers and expose several sets of APIs to LLM. 

## Blueprint
The first category is blueprint. For most of documents, a user only uses limited set of "design" patterns which we call blueprint. Blueprint specifies formatting for semantic elements of the document

```text
#### Character Names & Emphasis
- character.names: Mira, Percy (first mention)
- Format: <strong style="color:#278c28;"> (bold, deep green)
- Purpose: Visually distinguishes main characters on introduction
```    

The assumption is that a user will probably use the same formatting for the same semantic elements in similar documents. And if we can store and retrieve such elements, LLM can do better work on formatting. Needless to say that blueprints are generated by LLM automatically. 

## MetaPrompt

The **MetaPrompt** is the orchestration layer of the system — a prompt that reasons about other prompts.
It defines how prompts are created, selected, and executed based on user input and context.

At a high level:

* **Prompt (1)** defines the logic for composing and managing prompts.
* **Prompts (2)** are reusable behaviors or “skills.”
* **Prompt (3)** is generated by (1) — it takes user input (Prompt 4) and decides which of (2) to invoke.

In effect, the system uses prompts that *write* prompts that *choose* prompts.

This structure enables adaptive behavior: the app can adjust its responses or formatting strategies dynamically, guided by the same language model that drives its reasoning.

Unlike a single static prompt, a MetaPrompt encapsulates a *policy* for prompt selection.
It can take into account factors such as user actions, past context, or retrieved embeddings to select the most relevant sub-prompt.

For example, given a user history of editing patterns, a MetaPrompt might decide whether to:

* Apply an existing blueprint for a section,
* Generate a new variation if the style diverges, or
* Ask the user for clarification before committing a change.

Because the MetaPrompt itself is LLM-generated, it can evolve as new data or behaviors are introduced — essentially functioning as a *self-modifying control layer* for the system.

## Embeddings
Through the system, we are going to rely on LLM to provide keywords for storing and retrieving data. such as API for storing context defined as

```text
  {
    type: 'function' as const,
    function: {
      name: 'set_context',
      description: 'Set context value based on terms. The terms will be used to look up the context name, then store the value.',
      parameters: {
        type: 'object',
        properties: {
          terms: {
            type: 'array',
            keywords: { type: 'string' },
            description: 'Keywords to identify which context to set (e.g., ["document_name"], ["current_file"])'
          },
          value: {
            type: 'string',
            description: 'The value to store in the context'
          }
        },
        required: ['terms', 'value']
      }
    }
  }
```
runtime code goes through keyword, generates embedding for each and stores value using embedding as a key. 

The interesting question is how to find the best match given multiple keywords. The simplest is to find the best match for each keyword, and then lookup the best across all. However this approach does not account for history.

We want a simple way to map uncertain facts to the most likely outcome.

* Let **S** be a set of facts **F₁…Fₖ**, where each fact is a keyword with an associated probability.
* Our prior knowledge is a list of **rules** (statements) of the form **Sᵢ ⇒ Yᵢ**, where **Y** is the predicted outcome.
* **Goal:** given a new set of facts **S**, compute the most probable **Y**.

See [rulemodel.md](rulemodel.md) for the full fact model and implementation details.

To get rule model to work, we need a fixed set of keywords. In our case keywords are generated by LLM so we do not control the number. To fit keywords into the model we are going to define basis within keyword embedding vector space using [keywordbasis](keywordbasis.md)
